#set parameters values
set.seed(0)
bankroll<-1000
bet<- 100
trials<-5000
perc<-0.5
gambler<- function(B, b, N, p) {
game<-rep(NA,N) #this creates a vector of NAs
game[1]<-B
for (i in 2:N) {
hand <- game[i-1] + 2*b*rbinom(1, 1, prob = p) - b
game[i]<-hand
if (game[i]==0) {
return(game[1:i])
break
}
}
return(game)
}
simul.result<-replicate(trials, gambler(bankroll, bet, trials, perc)) # replicate the whole game and store results
p.bust.100<- sum(sapply(simul.result, function(x) length(x)<100))/trials #compute probability of being busted before 100th hand
p.bust.500<- sum(sapply(simul.result, function(x) length(x)<500))/trials #compute probability of being busted before 500th hand
mean.bust<-sapply(simul.result, function(x) length(x)) #copmute the length of each game
mean.bust<-mean(mean.bust[mean.bust<5000]) #compute the average number of hands before going busted, conditional of not having reached the upper limit of 5000
hand.100<-sapply(simul.result, "[", 100) #excract the 100th hand from each game
hand.100[is.na(hand.100)]<-0 #replace NAs with zeros to include those in the following computations
mean.100<-mean(hand.100, na.rm = TRUE)
var.100<-var(hand.100, na.rm = TRUE)
mean.100
var.100
hand.500<-sapply(simul.result, "[", 500)
hand.500[is.na(hand.500)]<-0
mean.500<-mean(hand.500, na.rm = TRUE)
var.500<-var(hand.500, na.rm = TRUE)
mean.500
var.500
perc.rou<-18/38 #set the new probability of winning
simul.roulette<-replicate(trials, gambler(bankroll, bet, trials, perc.rou))
p.bust.100.roulette<- sum(sapply(simul.roulette, function(x) length(x)<100))/trials
p.bust.100.roulette
p.bust.500.roulette<- sum(sapply(simul.roulette, function(x) length(x)<500))/trials
p.bust.500.roulette
mean.bust.roulette<-sapply(simul.roulette, function(x) length(x))
mean.bust.roulette<-mean(mean.bust.roulette[mean.bust.roulette<5000])
hand.100.roulette<-sapply(simul.roulette, "[", 100)
hand.100.roulette[is.na(hand.100.roulette)]<-0
mean.100.roulette<-mean(hand.100.roulette, na.rm = TRUE)
var.100.roulette<-var(hand.100.roulette, na.rm = TRUE)
mean.100.roulette
var.100.roulette
markov.trials<-100000
markov.p<-0.48
gambler.markov<- function(b, N, p) {
game<-rep(NA,N) #this creates a vector of NAs
z<-p #set initial probability
for (i in 1:N) {
win<-rbinom(1, 1, prob = z)
if (i ==1) { # specific "if" for the first hand as it does not have an antecendent
hand<-2*b*win - b
} else {
hand <- game[i-1] + 2*b*win - b
}
game[i]<-hand
if (win==1) { #"if/else" statements to update probability
z<-z + 0.01
if (z>1) {
z<-1
}
} else {
z<- p
}
}
return(game)
}
simul.markov<-replicate(100, gambler.markov(bet, markov.trials, markov.p)) #replicate the game 100 times
markov.ret<-simul.markov[dim(simul.markov)[1],] # markov returns
hist(markov.ret)
range(markov.ret)
mean(markov.ret)
head(simul.markov)
rng<- seq(from = 0.46, to = 0.48, by = 0.01)
markov.rng<-matrix(data = NA, nrow = length(rng), ncol = 2) #create empty matrix to store expected returns
rng
markov.rng
rng<- seq(from = 0.46, to = 0.47, by = 0.01)
markov.rng<-matrix(data = NA, nrow = length(rng), ncol = 2) #create empty matrix to store expected returns
for (t in 1:length(rng)) {
try.prob<-rng[t]
markov.rng[t,1]<-rng[t]
simul.markov<-replicate(100, gambler.markov(bet, markov.trials, try.prob))
markov.ret<-simul.markov[dim(simul.markov)[1],] # markov returns
markov.rng[t,2]<-mean(markov.ret)
}
markov.rng
rng<- seq(from = 0.46, to = 0.50, by = 0.01)
markov.rng<-matrix(data = NA, nrow = length(rng), ncol = 2) #create empty matrix to store expected returns
for (t in 1:length(rng)) {
try.prob<-rng[t]
markov.rng[t,1]<-rng[t]
simul.markov<-replicate(100, gambler.markov(bet, markov.trials, try.prob))
markov.ret<-simul.markov[dim(simul.markov)[1],] # markov returns
markov.rng[t,2]<-mean(markov.ret)
}
markov.rng
rng<- seq(from = 0.46, to = 0.50, by = 0.001)
markov.rng<-matrix(data = NA, nrow = length(rng), ncol = 2) #create empty matrix to store expected returns
for (t in 1:length(rng)) {
try.prob<-rng[t]
markov.rng[t,1]<-rng[t]
simul.markov<-replicate(100, gambler.markov(bet, markov.trials, try.prob))
markov.ret<-simul.markov[dim(simul.markov)[1],] # markov returns
markov.rng[t,2]<-mean(markov.ret)
}
markov.rng
min(markov.rng)
min(abs(markov.rng))
markov.rng[min(abs(markov.rng[,2]))
markov.rng[min(abs(markov.rng[,2]))
markov.rng[min(abs(markov.rng[,2]))]
min(abs(markov.rng[,2]))
markov.rng[markov.rng[,2]==min(abs(markov.rng[,2]))]
boot_ci = function(x, iter) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- mean(boot_sample)
}
boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
#boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
return(boot_se)
}
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata)
#mydata_sd = sd(mydata)
hist(mydata)
mydata_se=boot_ci(mydata, 10000) # why using the function here? shouldn't we
mydata_mean_ci <- mydata_mean + qt( c(0.025, 0.975), length(mydata) - 1) * mydata_se
mydata_sd = sd(mydata)
mydata_ci <- ci(mydata)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_mean_ci
mydata_boot_ci <- boot_ci(mydata, size)
mydata_boot_ci
mydata_se=boot_ci(mydata, 10000)
mydata_boot_ci <- boot_ci(mydata, size)
mydata_boot_ci
boot_ci = function(x, iter) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- mean(boot_sample)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
mydata_boot_ci <- boot_ci(mydata, size)
mydata_boot_ci
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
boot_ci = function(x, iter) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- apply(boot_sample, 1, mean)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size)
boot_ci = function(x, iter) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- fapply(boot_sample, mean)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- f(boot_sample)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
stat.int <- function(x) f(x, na.rm = TRUE)
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- stat.int(boot_sample)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
stat.int <- function(x) f(x, na.rm = TRUE)
funs <- c(stat.int)
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- lapply(boot_sample, funs)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
stat.int <- function(y) f(y, na.rm = TRUE)
funs <- c(stat.int)
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- lapply(boot_sample, funs)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
stat.int <- function(y) f(y, na.rm = TRUE)
funs <- c(stat.int(y))
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- lapply(boot_sample, funs)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
boot_ci = function(x, iter, f) {
#x: data to sample from
#iter: number of times you will draw a sample
stat.int <- function(y) f(y, na.rm = TRUE)
funs <- c(stat.int(y))
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- stat.int(boot_sample)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size, mean)
try <- function(stat) {
stat.int <- function(y) stat(y, na.rm = TRUE)
}
man.mean <- try(mean)
mean(c(1,2,3,4,5,6))
man.mean(c(1,2,3,4,5,6))
try <- function(stat) {
stat.int <- function(y) {
stat(y, na.rm = TRUE)
}
}
man.mean <- try(mean)
mean(c(1,2,3,4,5,6))
man.mean(c(1,2,3,4,5,6))
mean(c(1,2,3,4,5,6))
man.mean <- try(mean)
man.mena()
man.mean()
boot_ci = function(x, iter) {
#x: data to sample from
#iter: number of times you will draw a sample
boot_statistic = numeric()
for (i in 1:iter){
boot_sample = sample(x, size = length(x), replace=TRUE)
boot_statistic[i] <- mean(boot_sample)
}
#boot_se <- sd(boot_statistic) #why you take sd? shouldn't we just take the 0.025 and the 0.975 percentiles?
boot_ci <- quantile(boot_statistic, probs = c(0.025, 0.975))
#return(boot_se)
return(boot_ci)
}
# Assumptions
size = 10000
mean = 0
sd = 1
mydata <- rnorm(size, mean, sd)
mydata_mean = mean(mydata) #compute mean
mydata_sd = sd(mydata) #compute sd
hist(mydata)
#(gabri's edit) compute "real" CI (actually the same as Pablo's but I don't understand the steps he takes.)
mydata_lower <- mydata_mean - 1.96*(mydata_sd/sqrt(size))
mydata_upper <- mydata_mean + 1.96*(mydata_sd/sqrt(size))
mydata_ci <- c(mydata_lower, mydata_upper)
mydata_ci
mydata_boot_ci <- boot_ci(mydata, size)
mydata_boot_ci
#mydata_se=boot_ci(mydata, 10000)
#mydata_mean_ci <- mydata_mean + qt( c(0.025, 0.975), length(mydata) - 1) * mydata_se # why using the function here? shouldn't we get the CI from the bootstrap and compare it with the real one?
